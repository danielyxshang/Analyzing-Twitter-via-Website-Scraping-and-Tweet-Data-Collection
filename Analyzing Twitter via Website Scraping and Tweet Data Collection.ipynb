{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Twitter via Website Scraping and Tweet Data Collection (Data Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries and functions necessary for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to input the username and password without echoing on the screen\n",
    "from getpass import getpass\n",
    "\n",
    "# This function allows us to suspend the execution for the number of seconds we specified\n",
    "from time import sleep\n",
    "\n",
    "# These packages provide a series of tools used to initialize the browser, handle the error, get the response from the website, etc.\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "\n",
    "# Pandas is used to export the scraped data as a CSV file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function that is used to scrape the Tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_data(individual_tweet):\n",
    "    \n",
    "    # Extract the username. If username is failed to be extracted for some reasons, return nothing.\n",
    "    try:\n",
    "        username = individual_tweet.find_element_by_xpath('.//span').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    # Extract the user's handle. If handle is failed to be extracted for some reasons, return nothing.\n",
    "    try:\n",
    "        handle = individual_tweet.find_element_by_xpath('.//span[contains(text(), \"@\")]').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    # Extract the date the Tweet was posted. If the date is failed to be extracted for some reasons, return nothing.\n",
    "    try:\n",
    "        postdate = individual_tweet.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    # Extract the content of the Tweet. If the content is failed to be extracted for some reasons, return nothing.\n",
    "    try:\n",
    "        text = individual_tweet.find_element_by_xpath('.//div[2]/div[2]/div[1]').text\n",
    "    except NoSuchElementException:\n",
    "        return\n",
    "    \n",
    "    # Extract the number of reply. If the no one replied to the corresponding Tweet, return zero.\n",
    "    reply_count = individual_tweet.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "    if reply_count == '':\n",
    "        reply_count = '0'\n",
    "    else:\n",
    "        reply_count = reply_count\n",
    "    \n",
    "    # Extract the number of retweet. If the no one retweeted the corresponding Tweet, return zero.\n",
    "    retweet_count = individual_tweet.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "    if retweet_count == '':\n",
    "        retweet_count = '0'\n",
    "    else:\n",
    "        retweet_count = retweet_count\n",
    "    \n",
    "    # Extract the number of like. If the no one liked the corresponding Tweet, return zero.\n",
    "    like_count = individual_tweet.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "    if like_count == '':\n",
    "        like_count = '0'\n",
    "    else:\n",
    "        like_count = like_count\n",
    "    \n",
    "    # Assigned the information to a variable named 'tweet'\n",
    "    tweet = (username, handle, postdate, text, reply_count, retweet_count, like_count)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the HTML elements, identify the correct pattern, and input the necessary information to get ready for scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the username and password of a Twitter account\n",
    "user = getpass('Phone, email, or username: ')\n",
    "user_password = getpass('Password: ')\n",
    "\n",
    "# Initialize the Microsoft Edge browser\n",
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "driver = Edge(options=options)\n",
    "\n",
    "# Navigate to Twitter's login page and maximize the browser window. Maximizing is essential because Twitter adjust the feature available based on the size of the window\n",
    "driver.get('https://www.twitter.com/login')\n",
    "driver.maximize_window()\n",
    "\n",
    "# Inspect the HTML code elements on the login page, identify the correct input bracket, and input the username and password we defined previously\n",
    "username = driver.find_element_by_xpath('//input[@name=\"session[username_or_email]\"]')\n",
    "username.send_keys(user)\n",
    "password = driver.find_element_by_xpath('//input[@name=\"session[password]\"]')\n",
    "password.send_keys(user_password)\n",
    "\n",
    "# Get the response from Twitter login page and suspend for one second before the next execution to allow full website reaction\n",
    "password.send_keys(Keys.RETURN)\n",
    "sleep(1)\n",
    "\n",
    "# Inspect the element behind the search box, research the searching system and the proper keywords, and finally input the keywords and get the response\n",
    "search_key = driver.find_element_by_xpath('//input[@aria-label=\"Search query\"]')\n",
    "search_key.send_keys(\n",
    "    'vaccine (COVID OR COVID19 OR COVID-19 OR coronavirus OR corona) lang:en \\\n",
    "    until:2020-10-20 since:2019-12-01 -filter:replies')\n",
    "search_key.send_keys(Keys.RETURN)\n",
    "sleep(1)\n",
    "\n",
    "# Identify the element and click on 'Latest' to get the latest Tweets. Suspend execution for one second to allow full reaction\n",
    "driver.find_element_by_link_text('Latest').click()\n",
    "sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list and a set to store the Tweet data and Tweet id separately.\n",
    "data = []\n",
    "tweet_ids = set()\n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "scrolling = True\n",
    "\n",
    "while scrolling:\n",
    "    individual_tweets = driver.find_elements_by_xpath('//div[@data-testid=\"tweet\"]')\n",
    "    \n",
    "    # Since the previous content will not disappear if we keep scrolling down, we want to check only the last 15 Tweets to boost the processing speed.\n",
    "    # To ensure we do not repeatedly collect the same data, we assign a tweet_id to each tweet using its content. If the id has a match with previous id, we ignore it\n",
    "    for individual_tweet in individual_tweets[-15:]:\n",
    "        tweet = get_tweet_data(individual_tweet)\n",
    "        tweet_id = ''.join(tweet)\n",
    "        if tweet_id not in tweet_ids:\n",
    "            tweet_ids.add(tweet_id)\n",
    "            data.append(tweet)\n",
    "            \n",
    "    scroll_tried = 0\n",
    "    while True:\n",
    "        \n",
    "        # Ask the web driver to scroll down to the bottom of the page. Then, we suspend for 1.5 seconds to allow the website to load the new content\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(1.5)\n",
    "        \n",
    "        # Assign the current web page location to current_position variable\n",
    "        current_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        \n",
    "        # If the current position on the web page is the same as the previous position, we know that either we have reached the end of the content or the web page is not reacting\n",
    "        # In this case, we would allow three tries. If we are still at the same position after three tries, we would break out of this loop and go on. Otherwise, we would allow\n",
    "            # three 1.5 seconds for the page to fully load the content.\n",
    "        if last_position == current_position:\n",
    "            scroll_tried = scroll_tried + 1\n",
    "            if scroll_tried >= 3:\n",
    "                scrolling = False\n",
    "                break\n",
    "            else:\n",
    "                sleep(1.5)\n",
    "        \n",
    "        # If the last position is not the same as the current position, we know that the web page is loading new content properly. In this case, we assign the last position the\n",
    "            # current position, so that we can continue keeping track of the position.\n",
    "        else:\n",
    "            last_position = current_position\n",
    "            break\n",
    "    \n",
    "    # This is to limit the number of tweets we collect. Here, we are collecting 50 Tweets. To collect more Tweets, simply change it to a larger number\n",
    "    if len(data) >= 50:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data collected into a data frame, assign the proper column names, and export the data as a CSV file\n",
    "df = pd.DataFrame(data, columns = ['UserName', 'Handle', 'Timestamp', 'Text', 'Comments', 'Likes', 'Retweets'])\n",
    "df.to_csv(r'C:\\Users\\34527\\Desktop\\tweet.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
